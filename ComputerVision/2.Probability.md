# 2. Intro Prob.
- Random variables
- Joint Probability
- Marginalization : x,y input 이 있으면 p(x), p(y)를 각각 구하는 과정
   - Discrete한것은 Sumation을 Continuous 한 것은 integral을 실시
- Conditional Probability : 조건부 확률, $p(x|y)$ 에서 y가 특정 값 y* 일 때 x일 확률
$$ P_r(y|x) = P_r(x,y) / P_r(y) $$
  - Bayes rule : $P_r(x|y)$와 $P_r(y|x)$의 관계를 찾기 위함
      - 왜? : x(실제 상태) - y(센서 값) 관계라면 실제 확률은 $P_r(y|x)$ 의 관계로 나타나지만, 실제 상태 자체를 거꾸로 센서값을 통해 추정하는 경우 $P_r(x|y)$를 아는 경우가 일반적(온도가 3도이다 그러면 실제 온도는 $3 \pm 0.3$ 분포)이기 때문에 반대의 방법으로 $P_r(y|x)$ 추정하는 경우가 많기 때문에 중요      

      - 필요한 것들 : $P_r(x|y), Pr(y)$
      - posterior : $P_r(y|x)$, 추정이 필요 한 조건부 확률 -  x일때 y일 확률
      - prior : $P_r(y)$, 추정하고자 하는 y에 대해 아는 모든 것
      - likelihood : $P_r(x|y)$
      - evidence : $P_r(x)$
      예를 들면, x가 pixel값이고, y가 class라면
         - likelihood는 class가 'A'일 때 pixel값 Feature vector들의 분포가 측정 되겠지 : 학습 Data
         - prior는 Class 'A','B','C' 발생할 확률 : Uniform이어도 되고, 실제 분포가 있는 경우 넣어도 되고
         - evidence는 Pixel값의 분포 역시 uniform가정을 할 수 있는건가
    - independence : 독립이면 조건부 확률이 영향을 미치지 않지 서로.
    - Expectation : 확률 값의 평균이지 value*p(value)




# 3. Common probability distributions
- 다양한 distributions이 있으니 정의는 기억해두자
- 주로 Category랑 multivariate normal이 많이 쓰일 것이고, Dirichlet은 카테고리의 Parameter로 활용 될 수 있듯이 어떤 건 모델 자체를 fitting하는데 쓰이는 Form이고 어떤 것은 그 Form의 Parameter로 활용 될 수 있는 Form이다.
- Bernoulli distribution : 동전 던지기 (앞/뒤), binary trials
- Beta distribution : Bernoulli의 lambda (parameter)에 적합 [0,1]사이의 distribution을 정하며 alpha, beta를 필요로 한다.
- Normal-scaled inverse gamma idstribution은 $P_r(\mu,\alpha^2 )$ 로 정의 됨, 즉 normal disbution의 파라미터 피팅을 위해 생긴 놈이라고나 할까? 알필요가 있는지는 지켜보자
- Multivariate normal distribution : Covariance - Symmetric, Positive Definite matrix -> $z ^T \sum z$ is positive for any vector z
- Conjugacy : Conjugate관계에 있는 함수를 이용하면 Product 연산을 간략화 할 수 있고, Conjugate관계가 아닌 경우 이런 간략화는 어렵당

# 4. Fitting Probability models


# 5. The normal distribution



[] 23/11.2016
