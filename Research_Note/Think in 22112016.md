# Ultimate Global
The goal of Vision for Robotics is **make robot understand a scene by cameras and sensors and give people service**

##### What Service?
- Pick up and Delivery
   - SLAM
   - Object Detection & Recognition
   - Object Pose estimation
   - Grasp

SLAM : *Not* my interest
Object Detection & Recognition for known objects : V4R
Object Pose estimation for known objects : V4R
Object Detection & recognition & pose estimation for Grasping with Unknown? : Not performed in V4R, Who's doing this?

Grasping unknown and Learning and it can pick up and delivery well?

```{mermaid}
graph LR
A["1. Unknown objects in Scene"] --> B["2. Grasp Unknown"]
B --> C[3. Known Object Generation]
C --> D[4. Grasp Known]
```
Segmentation of unknown object but can be grasp object in Scene

나의 그림
Training Step
1. 책상에 물건이 있다. 처음엔 모든 물건이 뭔지 모른다. Segmentation도 제한적이다.
2. 로봇이 다가간다.
3. Pick up할 물체를 선택한다
   - 완전 모르는 물체를 선택한다
   - 완전 모르는 물체지만 뭔가 잡을 것 같은 느낌적인 느낌의 물건을 선택한다.
    (Transfer Learning)
   - 대충 아는 물체를 선택한다. (추가 학습 & Grasp 학습)
   - 완전 아는 물체를 선택한다 (Grasp policy 강화)
4. Grasp을 한다.
5. 모델을 돌려가며 학습을 한다.
6. 학습한 물체를 다른데 놓는다
7. 3-6을 반복한다.
8. 몇일 또는 몇번을. 로봇은 이제 책상에 모든 물체를 배웠고 잡을 줄 안다.
9. 각각 모델링 된 물체는 Human Robot interaction을 이용하여 Labeling한다.

Deployment Step
1. 책상에 아무렇게나 물건을 널부러 트려놓는다.
     - 가정1) 모든 물건을 사전 학습 되어있다.
     - 가정2) 사전 학습 된 물건과 아닌 물건이 섞여 있다.
     - 별상관이 없을 수 있다.
2. 특정 물건을 가져오라고 시킨다.
3. 로봇이 Grasp까지 잘 해서 가져다 준다.

Challenge
- 잘 모르는 물체지만 Grasp을 성공한 물체 중 비슷한 물체를 이용하여 쉽게 잡는 것.
- Grasp그 자체
- Unknown object의 segmentation : V4R
- 모델 돌려가며 학습 : Extension of V4R (with Mobile robot -> with grasp은 고전, 2011, 2016도 있긴 함)

과연 이방향으로 연구를 수행해보는 것이 의미 있는 지는 이야기 해 볼 필요가 있다.

Bin Picking을 하게 된다면
 - Step 1:Pose estimation, Known object with high accuracy RGB-D sensor [2017]
   Aldoma의 research를 적용 해 봄과 동시에 동일한 물체가 다수 겹쳐있는 상황에서의 Pose estimation 정확도를 따져 볼 수 있다.
   다만, 연구적으로는 어떻게 Performance를 비교 할 수 있을지 고민은 필요하다
   Ground Truth가 있는 Bin picking data set이 있는지, 없다면 어떻게 Generate 할건지도 문제 (하나씩 올려놓으면 accumulation하는 Approach? can I do this?)
   + Pipeline중 RGB Pipe를 R-CNN등의 State of the art method로 전환
     (Amazon picking challenge에서 활용)
   + 3D Pose Estimation은 aldoma 논문이 State of the art에 가까움
   분명 비슷한게 생긴 Object가 산재해 있을 때 Dealing 하는데 있어서 Aldoma 연구가 한계를 보일 수 있다. Hypothesis가 너무 많아. 검증도 어려워~ Something like that
   (Scence이 Hypothesis로 가득 찰 텐데 어떻게 이것도 Deal할지.. )


 - Step 2:Step1 Result + Manipulation Picking/Stowing Path planning[2018]
   Amazon Picking Challenge 참가 가능성 검토
   역시 manipulation과정에서도 연구 이슈는 많을 것으로 보이고, 로봇의 도메인에 따라 해결하는 접근론 역시 달라 지겠지. Pose estimation된 결과로 Picking 방식을 결정하는 방식 (주로 요즘은 Reinforcement learning이 사용 되는 부분) 의 발전을 꾀할 수도 있고, 역으로 Pose estimation을 좀 더 간략하게 하는 방법도 제안 할 수 있음 (Speed up)

 - Step 3:Unknown Object Pose Estimation and Picking[2019-end]
   + learning + modeling (already done with various researchers)

   I can give various objects to the robot. There are known and unknown objects. Robot just pick it up and if he knows, put it somewhere
   if he don't know, learning and modeling it and put it somewhere.
   He can ask people to label the new object

  - Step 4 : Automatic Labeling

  - 번외편 :  I want to compare Deep learning methods trained by feature based approaches when they can overcome the teachers


Okay, Let's follow the Aldoma's work and try Faster R-CNN Also
with Caffe, Let's make a plan from tomorrow.

Okay share my opinion with Markus.
